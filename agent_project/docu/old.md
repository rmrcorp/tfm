"""
        Router Sem√°ntico (LLM):
        Analiza la intenci√≥n real del usuario bas√°ndose en la conversaci√≥n
        para decidir si buscar en RAG, listar men√∫ o mantener el foco.
        """
messages = state["messages"]
last_user_msg = messages[-1].content

# 1. OBTENER CONTEXTO INMEDIATO (¬øQu√© dijo la IA justo antes?)
# Esto es CRUCIAL para distinguir si el usuario est√° respondiendo a una pregunta.
last_ai_msg = "Ninguno (Inicio de conversaci√≥n)"
ai_msgs = [m for m in messages if m.type == "ai"]
if ai_msgs:
    last_ai_msg = ai_msgs[-1].content

# 3. PROMPT DE CLASIFICACI√ìN
system_prompt = """Eres un experto analista de intenciones en un sistema de chat de Procesos de Negocio.
        Tu √∫nico trabajo es clasificar el √∫ltimo mensaje del usuario en una de estas 4 categor√≠as:

        1. 'provide_data': El usuario est√° proporcionando un dato corto (DNI, Nombre, Email, Motivo, Confirmaci√≥n) 
           que probablemente le ha pedido el Asistente en el mensaje anterior.
           Ejemplos: "Ruben", "12345678H", "baja voluntaria", "s√≠", "no".

        2. 'ask_capabilities': El usuario pregunta qu√© puede hacer el bot o qu√© procesos existen.
           Ejemplos: "¬øQu√© haces?", "Men√∫", "Lista de procesos", "Ayuda".

        3. 'query_process': El usuario quiere iniciar o consultar informaci√≥n sobre un proceso de negocio espec√≠fico.
           Ejemplos: "Quiero dar de alta", "C√≥mo doy de baja", "Requisitos para alta".

        4. 'general_chat': Saludos, despedidas o frases fuera de contexto de negocio.
           Ejemplos: "Hola", "Buenos d√≠as", "Gracias".

        5. ANTI-BUCLE: Antes de llamar a una herramienta, mira el historial. ¬øAcabas de llamarla con los mismos datos? Si es s√≠, DETENTE.
        6. FORMATO: NO escribas JSON en el chat. Ejecuta la herramienta de forma oculta (Native Tool Call).
        7. FINALIZACI√ìN: Si la herramienta devuelve "√âxito", TU TRABAJO HA TERMINADO. No vuelvas a llamar a la herramienta. Informa al usuario y calla.

        ANALIZA EL CONTEXTO:
        - √öltimo mensaje del Asistente: "{last_ai_msg}"
        - √öltimo mensaje del Usuario: "{user_input}"
        """

prompt = ChatPromptTemplate.from_template(system_prompt)

# Creamos la cadena
router_chain = prompt | llm_user_intention

# 4. EJECUTAR CLASIFICACI√ìN
print(f"üß† ROUTER: Analizando intenci√≥n de '{last_user_msg}'...")
try:
    decision = router_chain.invoke({
        "last_ai_msg": last_ai_msg,
        "user_input": last_user_msg
    })
    intent = decision.category
    print(f"DECISI√ìN LLM: {intent.upper()} (Raz√≥n: {decision.reasoning})")

except Exception as e:
    # Fallback por si el LLM falla al generar JSON (raro en Llama 3.1)
    print(f"!!! Error en Router LLM: {e}. Usando fallback heur√≠stico.")
    intent = "query_process"  # Asumimos b√∫squeda por defecto

 async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            print("Configurando MCPs...")
            langchain_tools = mapping_mcp_tools(session)
            print(f"MCPs conectados y mapeados: {len(langchain_tools)}")

            app = build_agent(langchain_tools)
            print("Sistema listo. Escribe 'salir' para terminar.")

            # this is the agent memory.
            config = {"configurable": {"thread_id": "user_session"}}

            while True:
                try:
                    user_input = input("Usuario: ")
                    if user_input.lower() in EXIT_INSTRUCTIONS: break
                    inputs = {"messages": [("human", user_input)]}
                    async for event in app.astream(inputs, config=config, stream_mode="values"):
                        await _show_last_ia_message(event)

                except KeyboardInterrupt:
                    print("Saliendo...")
                    break



