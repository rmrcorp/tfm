El problema que est치s sufriendo (confusi칩n entre "Alta" y "Baja" cuando ambos textos est치n presentes) es una limitaci칩n cl치sica de la "Ventana de Atenci칩n" de los modelos peque침os (8B). Un modelo de 8B par치metros tiene menos "capacidad cerebral" para sostener dos conceptos contradictorios en la cabeza y elegir el correcto; tiende a mezclarlos.

Un modelo m치s grande (14B, 32B o 70B) tiene una capacidad de razonamiento y seguimiento de instrucciones exponencialmente superior. Diferencia perfectamente el Alta de la Baja sin necesidad de prompts tan agresivos.

쯈u칠 modelo deber칤a usar? (La Gu칤a del Experto)
Dependiendo de la memoria RAM/VRAM (Tarjeta Gr치fica) que tengas en tu ordenador local, aqu칤 tienes mis recomendaciones para Ollama:

1. Si tienes 12GB - 16GB de VRAM (La opci칩n equilibrada)
游녤 Recomendaci칩n: Qwen 2.5 (14B) Actualmente, la familia Qwen 2.5 (de Alibaba) est치 superando a Llama 3.1 en casi todos los benchmarks de l칩gica y c칩digo. El modelo de 14B es mucho m치s listo que Llama 8B y cabe en tarjetas gr치ficas medias (RTX 3060/4060 de 12GB o Mac M1/M2/M3 con 16GB).

Comando: ollama pull qwen2.5:14b

2. Si tienes 24GB de VRAM o Mac con 32GB+ (La opci칩n profesional)
游녤 Recomendaci칩n: Qwen 2.5 (32B) Este es el "punto dulce" actual de la IA local. Tiene un nivel de inteligencia cercano a GPT-4 para tareas de negocio. Es extremadamente dif칤cil confundirlo. Si puedes correr este, tus problemas de "Alta vs Baja" desaparecer치n instant치neamente.

Comando: ollama pull qwen2.5:32b

3. La Bestia (Si tienes Mac Studio o Dual GPU)
游녤 Recomendaci칩n: Llama 3.3 (70B) Es el estado del arte. Razona mejor que muchos humanos. Pero necesitas una m치quina muy potente.

Comando: ollama pull llama3.3

C칩mo cambiar tu Agente para usar Qwen 2.5 (14B o 32B)
El cambio es trivial. Qwen funciona incre칤blemente bien con LangChain y Function Calling en las 칰ltimas versiones de Ollama.


El Candidato Ideal: Mistral NeMo (12B)Si quieres usar Mistral, este es el que debes instalar.Es un modelo de 12 Billones de par치metros creado conjuntamente por NVIDIA y Mistral AI.Por qu칠 funciona: Esos 4B par치metros extra respecto a Llama 3.1 (8B) marcan una diferencia enorme en capacidad de atenci칩n. Tiene una ventana de contexto de 128k tokens, lo que significa que es muy bueno leyendo documentos largos sin perderse.Hardware: Cabe perfectamente en una tarjeta gr치fica de 12GB (RTX 3060/4070) o en un Mac con 16GB de RAM.Comando: ollama pull mistral-nemo2. El "Est치ndar": Mistral 7B (v0.3)Veredicto: No te lo recomiendo para este problema.Raz칩n: Al tener 7B par치metros, tiene la misma limitaci칩n cognitiva que Llama 3.1. Es muy r치pido, pero ante instrucciones contradictorias ("Aqu칤 dice Alta, aqu칤 dice Baja"), se va a liar igual que te pasa ahora.3. El Peso Pesado: Mistral Small (24B)Veredicto: Una bestia en razonamiento, rivaliza con GPT-3.5/4.Contra: Requiere una tarjeta de 24GB VRAM (RTX 3090/4090) o un Mac con 32GB+ de RAM unificada. Es lento si no tienes buen hardware.Comando: ollama pull mistral-smallComparativa: Qwen 2.5 (14B) vs Mistral NeMo (12B)Dado que te recomend칠 Qwen 2.5 (14B) antes, aqu칤 tienes la diferencia clave para tu Agente BPM:Caracter칤sticaQwen 2.5 (14B)Mistral NeMo (12B)L칩gica Estricta游끥 Superior. Sigue instrucciones como un robot. Ideal para procesos de negocio r칤gidos.Muy buena, pero a veces intenta ser m치s "conversacional".Uso de Herramientas游볞 Excelente. Entiende JSON y esquemas Pydantic mejor que casi nadie en su peso.Bueno, pero a veces falla en el formato exacto de los argumentos.Idioma Espa침olMuy bueno.游볞 Nativo y fluido. Mistral suele redactar mejor en idiomas europeos.Separaci칩n de ContextoMuy alta. Distingue bien Alta de Baja.Alta. Su gran ventana de contexto ayuda mucho.Mi Recomendaci칩n DefinitivaPara un sistema que ejecuta procesos de negocio (BPM) donde un error es cr칤tico (borrar un usuario en vez de crearlo):Tu Opci칩n A (La m치s segura): qwen2.5:14b. Es m치s "fr칤o" y calculador, lo cual es perfecto para seguir reglas y manejar herramientas (MCP).Tu Opci칩n B (Si prefieres Mistral): mistral-nemo. Es una gran alternativa si notas que Qwen es demasiado "seco" o si Qwen te da problemas con el espa침ol (raro, pero posible).Prueba r치pida:Ejecuta esto en tu terminal y cambia el modelo en src/agent.py:Bashollama pull mistral-nemo
Y en el c칩digo:Pythonllm = ChatOllama(model="mistral-nemo", temperature=0)
Si Mistral NeMo sigue confundiendo Alta con Baja, entonces ve directo a Qwen 2.5 14B, que es actualmente el rey de la l칩gica en ese tama침o.